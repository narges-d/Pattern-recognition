# -*- coding: utf-8 -*-
"""BeysianPatternHomework2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1CvpqRoIZ-gJiHT0TluJSrxpcEtAh_Ugo
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from pandas.core.frame import DataFrame
from scipy.stats import zscore

# Commented out IPython magic to ensure Python compatibility.
# To have access to txt and csv file we should read data
# I want to read from dive
# So import drive from colab then mount it with the lacation

from google.colab import drive
drive.mount('/content/drive/')

# %cd /content/drive/My Drive/

# Sigmoid function is a function which change the number into [0,1] is a balance sigmoid
# This is calculate the probability of x
# This is calculate of y predict whic mean teta * 1 + teta * x1 + teta * x2

def sigmoid(x):
  return 1 /1 + np.exp(-x)

# Gradient ascent is a way to calculate mimimum of cost fuction
# This is calculate optimum teta wgic is min and make cost fuction optimum nd min
# Teta can be random and zero it select one point and go throw right point
# Cost is a list of cost for each teta and we can append it to list so we can see the progress
# Cost function of logistic regression described in notebook
# We should do the iteration for ties to achieve 0 value for gradient or finish limited

def gradient_ascent(x,y,learning_rate,iteration):
  m,n=x.shape
  teta=np.zero(n)
  costs=[]

  for i in range(iteration):
    h=sigmoid(np.dot(x,teta))
    gradient=np.dot(x.T,(y-h))/m
    teta+=learning_rate * gradient

    cost=(-1/m) * (y.T @  np.log(h) + (1-y) .T @ np.log(1-h))
    costs.append(cost)

  return teta,costs

# This is a function which just make and threshhold and separete data into 2 group

def predict(x,teta,threshhold=0.5):

  is_one= False

  probabilities=sigmoid(np.dot(x,teta))
  is_one=probabilities >= threshhold

  return is_one

# Accuracy sum 1 element for the matris predict commpared to threshhold where is it equal to y real
# At the last get mean of sum because we want % of totall and number between  and 1

def accuracy(x,y,teta):

  y_predict=predict(x,teta)
  return np.sum (y_predict == y) / len(y_predict)

# To read data first we have feature names we read it simply then column of class we change to list
# after that we add class string in the start of names because we shold apply list of name for column name of data frame

features=pd.read_csv("mushroom_feature_names.csv")
features= list(features['class'])
features.insert(0,'class')
#print(features)

# Then we read data txt includes features without header names
# It seperate data from spaces and put it in samples
# Then we should separate them into columns by , seperator
# Fro this approch we use data.str.split separator , and expand to multi column
# Finally name the columns features label

data=pd.read_csv("agaricus-lepiota.txt",sep="\s+",header=None)
#print(data[0])
data=data[0].str.split(",",expand=True)
#print(type(data))
data.columns=features
data

# Data info show the type of each column and numbers of entity and numbers of feature

data.info()

# For accessing the index string of data frame just have i in data
# It moves on index in data frame

for i in data:
  print("Unique data For",i,"is",data[i].nunique(),"Which are",data[i].unique())

""" Can you calculate any kind of distance between your samples now? why? Yes for example i can calcuate qulidisian distance because i can take diiference between same feaure per 2 samples and the then make them ^2 sum them and filnaly take ~ of them"""

# To do encode we make an instance of data then we go throw columns and check
# If column shape is object .fit_transform data string to numbers

label_encoder = LabelEncoder()

for column in data.columns:
    if data[column].dtype == 'object':
        data[column] = label_encoder.fit_transform(data[column])

print(data['cap-color'])
data

# To calculate varinace for data and see if there is useless feature

for i in data:
  if(data[i].var() < 0.01):
    print(data[i].var())
print("There is no data which variance are lower than 0.01")
data.var()

# Check is nan to see if we have nan values and we dont have any
nan_data=data.isna()
sum_nan=nan_data.sum()

print("The nan values are:",sum_nan)

# For answering This
# covariance matrix though, turns out to be singular. Now what is a singular matrix? What causes
# the covariance matrix derived from a feature matrix to be singular? name three reasons.
# Here we had veil-type which was all 0 in all features so it made determinan and our computational z so we drop it

data=data.drop("veil-type",axis=1)
features.remove('veil-type')
print(features)
data

# For spliting y label we should have data frame for y so we have to use 2 bracket
# And we drop data without label on axix =1 equals to y mean column

y=data[['class']]
x=data.drop('class',axis=1)

x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2)

"""what is the Bayes rule? How is it used in Bayesian classification?
What is each term of this rule called?
In this p(y|x)= p(x|y) p(y) / p(x) . in this formula the p(x|y) is the likelihood function and p(y) is prior and p(x) is the parameters that keep value small so it can be not consider. it is called beysian rule

What are the model parameters of LDA and QDA classifiers? What is the role of each parameter?
Between QDA and LDA, which one has has potentially more parameters? why?
We have 2 method linear discriminant analysis and quadratic discriminative analysis both are supervised learning for classification but their difference between nuber of parameter and qda has more parameters and they differ in data distribution consumering.
lda each class has mean vector per features.but all class share one covariance matrix for spread and correlation of all features
both have same priors qda has same class ean but qda has different covariance matrix for each class
"""

# Computing the elements of qda and lda method,first of all we should concat y train and x tarin
# So we can calulate per class in axis 1 mean columnly we concat it
# Then we gropuby labely for calculate mean and we separete 2 mean class 0 and 1 to list
# sigma for lda is one covarianse matris for both class
# Sigma for qda is one covarianse matrix per a class
# priorior y is number of y in each class/ percentage of 0 y in all and p of 1 y in all y
# We calculate .value to turn into array from list to number

xtrain_plus_ytrain=pd.concat([y_train,x_train], axis=1)

mean_list=np.split(xtrain_plus_ytrain.groupby('class').mean().values,2)
#(mean_list[0]).shape

sigma_lda=x_train.cov().values
sigma_qda=np.split(xtrain_plus_ytrain.groupby('class').cov().values,2)

pi_y_prorior=y_train.value_counts()/len(y_train)

# Print the len of xtrain , mean list and sigma sodoinvto check if multi matris multipliction cn be done or no
# I want to make sure what type of element in multi expression down

print("x_train shape:",x_train.shape)
print("x_train type:",type(x_train))
print("Mean class 0 shape:",mean_list[0].shape)
print("Mean class 0 type:",type(mean_list[0]))
print("covariance matris shape",len(np.linalg.pinv(sigma_lda)))
print("covariance matris type",type(np.linalg.pinv(sigma_lda)))

# 6499 * 21 - 21*21 - 21*6499
# Here is expression in the book / this | means detereminan matris covarianse / because we dont have inverse atris we set so inverse matris
# We should change the forat of x- mean to array because covarianse is array
# It is a litle different from book because of order to multipliction

from math import pi
def calculation(x,mean_list,sigma):

  sigma_inversion=np.linalg.pinv(sigma)
  n=len(features)-1

  zarib= 1. / (np.sqrt(2 * pi))** n * np.sqrt(np.linalg.det(sigma))
  pi_ygivenx= float(zarib * (np.exp((-1/2) * np.array(x - mean_list) @ sigma_inversion @  np.array(x - mean_list).T)))

  return pi_ygivenx

# LDA
# In this case we sent each sample to lda method and the x - mean of class and do calculation with the general covarinase for 2 class
# that fuction returns p(Y|x) then we multiply with p(y) prior and append to 2 list with the test data
# After that we compare 2 list with each other if the result for class 0 per sample is higher the class with be 0 and otherwise

pi_ygivenx_list_0=[]
pi_ygivenx_list_1=[]

count=0
for i in range(x_test.shape[0]):
  x=(np.array(list(x_test.iloc[i])))
  pi_ygivenx_list_0.append(calculation(x,mean_list[0],sigma_lda) * pi_y_prorior[0])
  pi_ygivenx_list_1.append(calculation(x,mean_list[1],sigma_lda) * pi_y_prorior[1])

print("LDA => P(y|x) for class 0:")
print((pi_ygivenx_list_0))

print("LDA => P(y|x) for class 1:")
print((pi_ygivenx_list_1))
print(type((pi_ygivenx_list_1)))

# This is rgmax function to do coparison
# remeber this is just for test data

y_calcu=[]
for i in range(len(pi_ygivenx_list_1)):
  if (pi_ygivenx_list_0[i:i+1][0]) > (pi_ygivenx_list_1[i:i+1][0]):
    y_calcu.append(0)
  else:
    y_calcu.append(1)

print("len pi_ygivenx_list_0 is",len(pi_ygivenx_list_0))
print("len y perdicted is",len(y_calcu))
print("len y test is",len(y_test))
print(y_calcu)
print(y_test)

# Previous code just for QDA

pi_ygivenx_list_0_qda=[]
pi_ygivenx_list_1_qda=[]

count=0
for i in range(x_test.shape[0]):
  x=(np.array(list(x_test.iloc[i])))
  pi_ygivenx_list_0_qda.append(calculation(x,mean_list[0],sigma_qda[0]) * pi_y_prorior[0])
  pi_ygivenx_list_1_qda.append(calculation(x,mean_list[1],sigma_qda[1]) * pi_y_prorior[1])

print("QDA => P(y|x) for class 0:")
print((pi_ygivenx_list_0_qda))

print("QDA => P(y|x) for class 1:")
print((pi_ygivenx_list_1_qda))
print(type((pi_ygivenx_list_1_qda)))

# Previous code just for QDA

y_calcu_qda=[]
for i in range(len(pi_ygivenx_list_1_qda)):
  if (pi_ygivenx_list_0_qda[i:i+1][0]) > (pi_ygivenx_list_1_qda[i:i+1][0]):
    y_calcu_qda.append(0)
  else:
    y_calcu_qda.append(1)

print("QDA => len pi_ygivenx_list_0 is",len(pi_ygivenx_list_0_qda))
print("QDA => len y perdicted is",len(y_calcu_qda))
print("QDA => len y test is",len(y_test))
print(y_calcu_qda)
print(y_test)

# Here is Method to calculate accuracy check where teh results are same of test label and count and / all

accuracy_lda=0
accuracy_qda=0
sum_lda=0
sum_qda=0

for i in range(len(y_test)):
  if ((list(y_test['class'])[i]) == y_calcu[i]):
    sum_lda +=1

  if ((list(y_test['class'])[i]) == y_calcu_qda[i]):
    sum_qda +=1

accuracy_lda=(sum_lda)/len(y_test)
accuracy_qda=(sum_qda)/len(y_test)

print("Accuracy LDA for test:",accuracy_lda)
print("Accuracy QDA for test:",accuracy_qda)

"""What causes the Covariance matrix to be singular? name 3 reason
1- one feature is mutipliction of other feature
2- we dont have much sample
3- we have more feature than saple
4 - we dont have mature data set or we have sae feature for one or more features
"""

# This is a method to calculate confusion matrix and precision and f1score and recall and number of missed valuebale for classification

def consion(y_real,y_pred):
 true_positive=0
 false_positive=0
 false_negative=0
 true_negetive=0

 for i in range(len(y_real)):
    if ((list(y_real['class'])[i]) == y_pred[i] and y_pred[i]==1):
      true_positive+=1
    if ((list(y_real['class'])[i]) == y_pred[i] and y_pred[i]==0):
      true_negetive+=1
    if ((list(y_real['class'])[i]) != y_pred[i] and y_pred[i]==0):
      false_negative+=1
    if ((list(y_real['class'])[i]) != y_pred[i] and y_pred[i]==1):
      false_positive+=1

 matrix = [[0, 0],[0, 0]]
 matrix[0][0] =true_positive
 matrix[0][1] =false_positive
 matrix[1][0] =false_negative
 matrix[1][1] =true_negetive

 print([['true_positive', 'false_positive']])
 print([['false_negative','true_negetive']])
 print(np.array(matrix))

 return matrix

# this is print method for both lda and qda

print("Confusion matrix for LDA:")
matrix_lda=consion(y_test,y_calcu)
print()

print("Precision for LDA:")
precision_lda=matrix_lda[0][0]/(matrix_lda[0][0] + matrix_lda[0][1])
print(precision_lda)
print()

print("Recall for LDA:")
recall_lda=matrix_lda[0][0]/(matrix_lda[0][0] + matrix_lda[1][0])
print(recall_lda)
print()

print("f1-score for LDA:")
f1score_lda=(precision_lda * recall_lda) / (recall_lda + precision_lda)
print(f1score_lda)
print()

print("Number of misclasifiered in LDA:")
missval_lda= matrix_lda[0][1] + matrix_lda[1][0]
print(missval_lda)
print()

print("Consusion matrix for QDA:")
matrix_qda=consion(y_test,y_calcu_qda)
print()

print("Precision for QDA:")
precision_qda=matrix_qda[0][0]/(matrix_qda[0][0] + matrix_qda[0][1])
print(precision_qda)
print()

print("Recall for QDA:")
recall_qda=matrix_qda[0][0]/(matrix_qda[0][0] + matrix_qda[1][0])
print(recall_qda)
print()

print("f1-score for QDA:")
f1score_qda=(precision_qda* recall_qda) / (recall_qda + precision_qda)
print(f1score_qda)
print()

print("Number of misclasifiered in QDA:")
missval_qda= matrix_qda[0][1] + matrix_qda[1][0]
print(missval_qda)
print()

"""From confusion matrix it can be derived that in QDA the numbers of false is more less than LDA and also the number of true negetive is ucher than the number of true positive and i dont know why

Which one is more
important in the task of classifying poisonous mushrooms? I think fase negetaive is more important to detect nonclifier because it is bigger

finally report the model parameters for both classifiers. How many parameters are there? there are more paramteres for qda but at all we have mean parameter for mu per each class and we have covariance for both LDA and QDA but in QDA we have sigma or covariance matrix for each class it gave us good result and the result of QDA is better at all.
but in logistic regression familly number of parameters depend of the number of features because if n feature we have n phi or teta tha multiply to feature and also we have one bias so we have n + 1 parametr . for lda we have c number of meu and one covariance matrix so we have c +1 parameter which c is number of class and in qda we have c meu and c covarianse matrix so we have 2*c parameters
"""